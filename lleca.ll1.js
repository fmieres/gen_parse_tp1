/**
 * LL(1) parser generated by the Syntax tool.
 *
 * https://www.npmjs.com/package/syntax-cli
 *
 *   npm install -g syntax-cli
 *
 *   syntax-cli --help
 *
 * To regenerate run:
 *
 *   syntax-cli \
 *     --grammar ~/path-to-grammar-file \
 *     --mode LL1 \
 *     --output ~/path-to-output-parser-file.js
 */

'use strict';

let yytext;
let yyleng;
let __;

const EOF = '$';

const ps = [[-1],
[[]],
[["0","1"]],
[["2","11"]],
[[]],
[["2","3"]],
[["6","13","4","12"]],
[[]],
[["4","5"]],
[["14"]],
[["15"]],
[["16"]],
[["17"]],
[["11"]],
[["18"]],
[["7","11"]],
[["17"]],
[["19"]],
[["10","19","20"]],
[[]],
[["22","8","21"]],
[[]],
[["9","6"]],
[[]],
[["9","6","23"]],
[[]],
[["25","6","24"]]];
const tks = {"IDENTIFIER":"11","SYM_PIPE":"12","SYM_ARROW":"13","KEY_ID":"14","KEY_STRING":"15","KEY_NUM":"16","STRING":"17","KEY_UNDERSCORE":"18","NUMBER":"19","SYM_DOLLAR":"20","SYM_LPAREN":"21","SYM_RPAREN":"22","SYM_COMMA":"23","SYM_LBRACKET":"24","SYM_RBRACKET":"25","$":"26"};
const tbl = {"0":{"11":"2","26":"1"},"1":{"11":"3"},"2":{"11":"4","12":"5","26":"4"},"3":{"12":"6"},"4":{"11":"8","13":"7","14":"8","15":"8","16":"8","17":"8"},"5":{"11":"13","14":"9","15":"10","16":"11","17":"12"},"6":{"11":"15","17":"16","18":"14","19":"17","20":"18"},"7":{"11":"19","12":"19","21":"20","22":"19","23":"19","25":"19","26":"19"},"8":{"11":"22","17":"22","18":"22","19":"22","20":"22","22":"21"},"9":{"22":"23","23":"24"},"10":{"11":"25","12":"25","22":"25","23":"25","24":"26","25":"25","26":"25"}};

const s = [];

let tokenizer;
/**
 * Generic tokenizer used by the parser in the Syntax tool.
 *
 * https://www.npmjs.com/package/syntax-cli
 *
 * See `--custom-tokinzer` to skip this generation, and use a custom one.
 */

const lexRules = [[/^\/\/.*/, function() { /* skip comments */ }],
[/^\/\*(.|\s)*?\*\//, function() { /* skip comments */ }],
[/^\s+/, function() { /* skip whitespace */ }],
[/^\_/, function() { return 'KEY_UNDERSCORE' }],
[/^\ID/, function() { return 'KEY_ID' }],
[/^\STRING/, function() { return 'KEY_STRING' }],
[/^\NUM/, function() { return 'KEY_NUM' }],
[/^\=>/, function() { return 'SYM_ARROW' }],
[/^\|/, function() { return 'SYM_PIPE' }],
[/^\$/, function() { return 'SYM_DOLLAR' }],
[/^\(/, function() { return 'SYM_LPAREN' }],
[/^\)/, function() { return 'SYM_RPAREN' }],
[/^\[/, function() { return 'SYM_LBRACKET' }],
[/^\]/, function() { return 'SYM_RBRACKET' }],
[/^,/, function() { return 'SYM_COMMA' }],
[/^(\d+(\.\d+)?)/, function() { return 'NUMBER' }],
[/^"[^"]*"/, function() { yytext = yytext.slice(1, -1); return 'STRING'; }],
[/^'[^']*'/, function() { yytext = yytext.slice(1, -1); return 'STRING'; }],
[/^[a-zA-Z0-9_]+/, function() { return 'IDENTIFIER' }]];
const lexRulesByConditions = {"INITIAL":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]};

const EOF_TOKEN = {
  type: EOF,
  value: '',
};

tokenizer = {
  initString(string) {
    this._string = string;
    this._cursor = 0;

    this._states = ['INITIAL'];
    this._tokensQueue = [];

    this._currentLine = 1;
    this._currentColumn = 0;
    this._currentLineBeginOffset = 0;

    /**
     * Matched token location data.
     */
    this._tokenStartOffset = 0;
    this._tokenEndOffset = 0;
    this._tokenStartLine = 1;
    this._tokenEndLine = 1;
    this._tokenStartColumn = 0;
    this._tokenEndColumn = 0;

    return this;
  },

  /**
   * Returns tokenizer states.
   */
  getStates() {
    return this._states;
  },

  getCurrentState() {
    return this._states[this._states.length - 1];
  },

  pushState(state) {
    this._states.push(state);
  },

  begin(state) {
    this.pushState(state);
  },

  popState() {
    if (this._states.length > 1) {
      return this._states.pop();
    }
    return this._states[0];
  },

  getNextToken() {
    // Something was queued, return it.
    if (this._tokensQueue.length > 0) {
      return this._toToken(this._tokensQueue.shift());
    }

    if (!this.hasMoreTokens()) {
      return EOF_TOKEN;
    }

    let string = this._string.slice(this._cursor);
    let lexRulesForState = lexRulesByConditions[this.getCurrentState()];

    for (let i = 0; i < lexRulesForState.length; i++) {
      let lexRuleIndex = lexRulesForState[i];
      let lexRule = lexRules[lexRuleIndex];

      let matched = this._match(string, lexRule[0]);

      // Manual handling of EOF token (the end of string). Return it
      // as `EOF` symbol.
      if (string === '' && matched === '') {
        this._cursor++;
      }

      if (matched !== null) {
        yytext = matched;
        yyleng = yytext.length;
        let token = lexRule[1].call(this);

        if (!token) {
          return this.getNextToken();
        }

        // If multiple tokens are returned, save them to return
        // on next `getNextToken` call.

        if (Array.isArray(token)) {
          const tokensToQueue = token.slice(1);
          token = token[0];
          if (tokensToQueue.length > 0) {
            this._tokensQueue.unshift(...tokensToQueue);
          }
        }

        return this._toToken(token, yytext);
      }
    }

    if (this.isEOF()) {
      this._cursor++;
      return EOF_TOKEN;
    }

    this.throwUnexpectedToken(
      string[0],
      this._currentLine,
      this._currentColumn
    );
  },

  /**
   * Throws default "Unexpected token" exception, showing the actual
   * line from the source, pointing with the ^ marker to the bad token.
   * In addition, shows `line:column` location.
   */
  throwUnexpectedToken(symbol, line, column) {
    const lineSource = this._string.split('\n')[line - 1];
    let lineData = '';

    if (lineSource) {
      const pad = ' '.repeat(column);
      lineData = '\n\n' + lineSource + '\n' + pad + '^\n';
    }

    throw new SyntaxError(
      `${lineData}Unexpected token: "${symbol}" ` +
      `at ${line}:${column}.`
    );
  },

  getCursor() {
    return this._cursor;
  },

  getCurrentLine() {
    return this._currentLine;
  },

  getCurrentColumn() {
    return this._currentColumn;
  },

  _captureLocation(matched) {
    const nlRe = /\n/g;

    // Absolute offsets.
    this._tokenStartOffset = this._cursor;

    // Line-based locations, start.
    this._tokenStartLine = this._currentLine;
    this._tokenStartColumn =
      this._tokenStartOffset - this._currentLineBeginOffset;

    // Extract `\n` in the matched token.
    let nlMatch;
    while ((nlMatch = nlRe.exec(matched)) !== null) {
      this._currentLine++;
      this._currentLineBeginOffset = this._tokenStartOffset + nlMatch.index + 1;
    }

    this._tokenEndOffset = this._cursor + matched.length;

    // Line-based locations, end.
    this._tokenEndLine = this._currentLine;
    this._tokenEndColumn = this._currentColumn =
      (this._tokenEndOffset - this._currentLineBeginOffset);
  },

  _toToken(tokenType, yytext = '') {
    return {
      // Basic data.
      type: tokenType,
      value: yytext,

      // Location data.
      startOffset: this._tokenStartOffset,
      endOffset: this._tokenEndOffset,
      startLine: this._tokenStartLine,
      endLine: this._tokenEndLine,
      startColumn: this._tokenStartColumn,
      endColumn: this._tokenEndColumn,
    };
  },

  isEOF() {
    return this._cursor === this._string.length;
  },

  hasMoreTokens() {
    return this._cursor <= this._string.length;
  },

  _match(string, regexp) {
    let matched = string.match(regexp);
    if (matched) {
      // Handle `\n` in the matched token to track line numbers.
      this._captureLocation(matched[0]);
      this._cursor += matched[0].length;
      return matched[0];
    }
    return null;
  },
};

const yyparse = {
  parse(string) {
    yyparse.onParseBegin(string);

    if (!tokenizer) {
      throw new Error(`Tokenizer instance wasn't specified.`);
    }

    tokenizer.initString(string);

    s.length = 0;
    s.push(EOF, '0');

    let t = tokenizer.getNextToken();
    let to = null;
    let tt = null;

    do {
      to = s.pop();
      tt = tks[t.type];

      if (to === tt) {
        t = tokenizer.getNextToken();
        continue;
      }

      der(to, t, tt);
    } while (tokenizer.hasMoreTokens() || s.length > 1);

    while (s.length !== 1) {
      der(s.pop(), t, tt);
    }

    if (s[0] !== EOF || t.type !== EOF) {
      parseError(`stack is not empty: ${s}, ${t.value}`);
    }

    return true;
  },

  setTokenizer(customTokenizer) {
    tokenizer = customTokenizer;
    return yyparse;
  },

  getTokenizer() {
    return tokenizer;
  },

  onParseBegin(string) {},
  onParseEnd(parsed) {},
};


    // Can be "require" statments, or direct declarations.
  

function der(to, t, tt) {
  let npn = tbl[to][tt];
  if (!npn) {
    unexpectedToken(t);
  }
  s.push(...ps[npn][0]);
}

function unexpectedToken(token) {
  if (token.type === EOF) {
    unexpectedEndOfInput();
  }

  tokenizer.throwUnexpectedToken(
    token.value,
    token.startLine,
    token.startColumn
  );
}

function unexpectedEndOfInput() {
  parseError(`Unexpected end of input.`);
}

function parseError(message) {
  throw new SyntaxError(message);
}

module.exports = yyparse;